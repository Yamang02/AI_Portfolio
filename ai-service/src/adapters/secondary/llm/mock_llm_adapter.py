"""
Mock LLM Adapter - Secondary Adapter (Hexagonal Architecture)
Í∞úÎ∞ú/ÌÖåÏä§Ìä∏Ïö© LLM Íµ¨ÌòÑÏ≤¥
"""

import asyncio
import random
from typing import Dict, Any, Optional

from ....core.ports.llm_port import LLMPort
from ....core.domain.models import RAGQuery


class MockLLMAdapter(LLMPort):
    """Í∞úÎ∞ú/ÌÖåÏä§Ìä∏Ïö© Mock LLM Ïñ¥ÎåëÌÑ∞"""
    
    def __init__(self):
        self.model_name = "mock-llm-hexagonal-v1"
        self._available = True
    
    async def generate_response(
        self, 
        query: str, 
        context: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """Í∏∞Î≥∏ ÏùëÎãµ ÏÉùÏÑ±"""
        # ÏùëÎãµ ÏãúÍ∞Ñ ÏãúÎÆ¨Î†àÏù¥ÏÖò
        await asyncio.sleep(random.uniform(0.3, 1.0))
        
        base_responses = [
            f"'{query}'Ïóê ÎåÄÌï¥ ÎãµÎ≥ÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.",
            f"ÏßàÎ¨∏Ìï¥Ï£ºÏã† '{query}' Í¥ÄÎ†®ÌïòÏó¨ ÏÑ§Î™ÖÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.",
            f"'{query}'Ïóê ÎåÄÌïú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌï¥ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.",
        ]
        
        response = random.choice(base_responses)
        
        # Ïª®ÌÖçÏä§Ìä∏Í∞Ä ÏûàÏúºÎ©¥ ÌôúÏö©
        if context and len(context) > 10:
            context_preview = context[:150] + "..." if len(context) > 150 else context
            response += f"\n\nÏ†úÍ≥µÎêú Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú:\n{context_preview}"
        
        # ÏßàÎ¨∏ Ïú†ÌòïÎ≥Ñ Ï∂îÍ∞Ä Ï†ïÎ≥¥
        if any(word in query.lower() for word in ['Í∏∞Ïà†', 'tech', 'Ïä§ÌÉù']):
            response += "\n\nÏ£ºÏöî Í∏∞Ïà† Ïä§ÌÉù: React, FastAPI, PostgreSQL, Qdrant, Python Îì±ÏùÑ ÌôúÏö©Ìï©ÎãàÎã§."
        
        if any(word in query.lower() for word in ['ÌîÑÎ°úÏ†ùÌä∏', 'project']):
            response += "\n\nÏù¥ Ìè¨Ìä∏Ìè¥Î¶¨Ïò§Îäî AI Í∏∞Î∞ò RAG ÏãúÏä§ÌÖúÏùÑ Íµ¨ÌòÑÌïú ÌîÑÎ°úÏ†ùÌä∏ÏûÖÎãàÎã§."
        
        return response
    
    async def generate_rag_response(
        self, 
        rag_query: RAGQuery, 
        context: str
    ) -> str:
        """RAG Ï†ÑÏö© ÏùëÎãµ ÏÉùÏÑ±"""
        await asyncio.sleep(random.uniform(0.5, 1.2))
        
        question = rag_query.question
        
        # RAG Ï†ÑÏö© ÏùëÎãµ ÌÖúÌîåÎ¶ø
        rag_responses = [
            f"'{question}'Ïóê ÎåÄÌï¥ Í¥ÄÎ†® ÏûêÎ£åÎ•º Í≤ÄÌÜ†Ìïú Í≤∞Í≥º Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:",
            f"Ï†úÍ≥µÎêú Î¨∏ÏÑúÎì§ÏùÑ Î∂ÑÏÑùÌïòÏó¨ '{question}'Ïóê ÎåÄÌï¥ ÎãµÎ≥ÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§:",
            f"'{question}'ÏôÄ Í¥ÄÎ†®Îêú Ï†ïÎ≥¥Î•º Ï¢ÖÌï©ÌïòÎ©¥:",
        ]
        
        base_response = random.choice(rag_responses)
        
        # Ïª®ÌÖçÏä§Ìä∏ ÏöîÏïΩ
        if context:
            context_lines = context.split('\n')[:5]  # Ï≤òÏùå 5Ï§ÑÎßå
            context_summary = '\n'.join(context_lines)
            
            base_response += f"\n\nüìö Ï∞∏Ï°∞ Ï†ïÎ≥¥:\n{context_summary}"
            
            if len(context) > 200:
                base_response += "\n\n... (Îçî ÎßéÏùÄ Í¥ÄÎ†® Ï†ïÎ≥¥Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§)"
        
        # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌôúÏö©
        if rag_query.context_hint:
            base_response += f"\n\nüí° Ïª®ÌÖçÏä§Ìä∏ ÌûåÌä∏: {rag_query.context_hint}"
        
        base_response += f"\n\nüîç Í≤ÄÏÉâÎêú Î¨∏ÏÑú Ïàò: {rag_query.max_results}Í∞ú"
        
        return base_response
    
    def is_available(self) -> bool:
        """ÏÑúÎπÑÏä§ ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä"""
        return self._available
    
    def get_model_info(self) -> Dict[str, Any]:
        """Î™®Îç∏ Ï†ïÎ≥¥"""
        return {
            "model_name": self.model_name,
            "type": "mock",
            "version": "1.0.0",
            "architecture": "hexagonal",
            "capabilities": [
                "text-generation", 
                "context-aware", 
                "rag-optimized"
            ],
            "limitations": [
                "Mock implementation",
                "No actual AI processing",
                "Development/testing only"
            ],
            "performance": {
                "avg_response_time_ms": "300-1000",
                "max_context_length": "unlimited",
                "supported_languages": ["ÌïúÍµ≠Ïñ¥", "English"]
            }
        }