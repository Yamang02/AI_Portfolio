"""
Execute RAG Query Use Case
RAG Query ì‹¤í–‰ ìœ ìŠ¤ì¼€ì´ìŠ¤

ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë¬¸ì„œ ê²€ìƒ‰ í›„ AI ë‹µë³€ì„ ìƒì„±í•˜ëŠ” Use Caseì…ë‹ˆë‹¤.
"""

import logging
from typing import Dict, Any, Tuple, List
from domain.services.retrieval_service import RetrievalService
from domain.services.generation_service import GenerationService
from domain.services.document_management_service import DocumentService
from domain.services.query_template_service import QueryTemplateService
from domain.entities.query import Query

logger = logging.getLogger(__name__)


class ExecuteRAGQueryUseCase:
    """RAG Query ì‹¤í–‰ ìœ ìŠ¤ì¼€ì´ìŠ¤"""
    
    def __init__(
        self,
        retrieval_service: RetrievalService,
        generation_service: GenerationService,
        document_service: DocumentService,
        query_template_service: QueryTemplateService = None
    ):
        self.retrieval_service = retrieval_service
        self.generation_service = generation_service
        self.document_service = document_service
        self.query_template_service = query_template_service or QueryTemplateService()
        logger.info("âœ… ExecuteRAGQueryUseCase initialized")
    
    def execute(
        self,
        question: str,
        max_sources: int = 3,
        similarity_threshold: float = 0.1
    ) -> Dict[str, Any]:
        """RAG Query ì‹¤í–‰"""
        try:
            if not question.strip():
                return {
                    "success": False,
                    "error": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”",
                    "answer": "",
                    "sources": ""
                }
            
            # Query ì—”í‹°í‹° ìƒì„±
            query = Query(
                text=question,
                query_type="RAG_QUESTION",
                max_results=max_sources,
                similarity_threshold=similarity_threshold
            )
            
            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            search_results = self.retrieval_service.search_similar_chunks(
                query=query,
                top_k=max_sources,
                similarity_threshold=similarity_threshold
            )
            
            if not search_results:
                return {
                    "success": True,
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•œ í›„ ì„ë² ë”©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
                    "sources": "ğŸ“­ ë²¡í„°ìŠ¤í† ì–´ì— ì²­í¬ê°€ ì—†ê±°ë‚˜ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "query_id": str(query.query_id),
                    "search_results_count": 0
                }
            
            # RAG ì‘ë‹µ ìƒì„±
            rag_response = self.generation_service.generate_rag_response(
                query=query,
                search_results=search_results,
                max_sources=max_sources
            )
            
            # ì¶œì²˜ ì •ë³´ í¬ë§·íŒ…
            sources_text = self._format_sources(search_results[:max_sources])
            
            return {
                "success": True,
                "answer": rag_response.answer,
                "sources": sources_text,
                "query_id": str(query.query_id),
                "response_id": str(rag_response.rag_response_id),
                "confidence_score": rag_response.confidence_score,
                "processing_time_ms": rag_response.processing_time_ms,
                "search_results_count": len(search_results),
                "used_sources_count": len(search_results[:max_sources])
            }
            
        except Exception as e:
            logger.error(f"RAG Query ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "answer": f"âŒ Query ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": "ì˜¤ë¥˜ë¡œ ì¸í•´ ì¶œì²˜ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
    
    def _format_sources(self, search_results) -> str:
        """ì¶œì²˜ ì •ë³´ í¬ë§·íŒ…"""
        if not search_results:
            return "ğŸ“­ ì‚¬ìš©ëœ ì¶œì²˜ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        sources_parts = ["ğŸ“š **ì°¸ì¡°ëœ ì²­í¬ë“¤:**\n"]
        
        for i, result in enumerate(search_results):
            chunk = result.chunk
            similarity = result.similarity_score
            relevance = result.get_relevance_level()
            
            # ì²­í¬ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 150ì)
            content_preview = chunk.content[:150] + "..." if len(chunk.content) > 150 else chunk.content
            
            source_info = f"""
**ì²­í¬ {i+1}** (ìœ ì‚¬ë„: {similarity:.3f}, ê´€ë ¨ì„±: {relevance})
- **ì†Œì† ë¬¸ì„œ ID**: {str(chunk.document_id)[:8]}...
- **ì²­í¬ ID**: {str(chunk.chunk_id)[:8]}...
- **ì²­í¬ ì¸ë±ìŠ¤**: {chunk.chunk_index}
- **ì²­í¬ í¬ê¸°**: {len(chunk.content)} ê¸€ì
- **ì²­í¬ ë‚´ìš©**: {content_preview}
---"""
            sources_parts.append(source_info)
        
        return "\n".join(sources_parts)
    
    def get_sample_queries_for_loaded_documents(self) -> List[Dict[str, Any]]:
        """ë¡œë“œëœ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒ˜í”Œ ì¿¼ë¦¬ ìƒì„±"""
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸: ê¸°ë³¸ ìƒ˜í”Œ ì¿¼ë¦¬ ë°˜í™˜ (ë¬¸ì„œ ë¡œë“œ ìƒíƒœ ë¬´ê´€)
            sample_queries = [
                {
                    "query": "AI í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ê¸°ìˆ  ìŠ¤íƒì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "expected_type": "PROJECT",
                    "confidence": 0.95,
                    "reasoning": "í”„ë¡œì íŠ¸ì˜ ê¸°ìˆ  ìŠ¤íƒì„ ë¬»ëŠ” ì§ˆë¬¸ (Mock LLM ë¶„ë¥˜)",
                    "source_document": "AI Portfolio Project"
                },
                {
                    "query": "í—¥ì‚¬ê³ ë‚  ì•„í‚¤í…ì²˜ë¥¼ ì„ íƒí•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                    "expected_type": "EXPERIENCE",
                    "confidence": 0.91,
                    "reasoning": "ì•„í‚¤í…ì²˜ ì„ íƒ ê²½í—˜ì„ ë¬»ëŠ” ì§ˆë¬¸ (Mock LLM ë¶„ë¥˜)",
                    "source_document": "Architecture Q&A"
                },
                {
                    "query": "RAG ì‹œìŠ¤í…œì—ì„œ ë²¡í„° ê²€ìƒ‰ì€ ì–´ë–»ê²Œ êµ¬í˜„í–ˆë‚˜ìš”?",
                    "expected_type": "TECHNICAL_SKILL",
                    "confidence": 0.89,
                    "reasoning": "êµ¬ì²´ì  ê¸°ìˆ  êµ¬í˜„ì„ ë¬»ëŠ” ì§ˆë¬¸ (Mock LLM ë¶„ë¥˜)",
                    "source_document": "RAG System Q&A"
                },
                {
                    "query": "ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ë°°ìš´ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "expected_type": "EXPERIENCE", 
                    "confidence": 0.88,
                    "reasoning": "í”„ë¡œì íŠ¸ ê²½í—˜ê³¼ í•™ìŠµì„ ë¬»ëŠ” ì§ˆë¬¸ (Mock LLM ë¶„ë¥˜)",
                    "source_document": "Learning Experience"
                }
            ]
            
            return sample_queries
            
        except Exception as e:
            logger.error(f"Error generating sample queries: {e}")
            return []
    
