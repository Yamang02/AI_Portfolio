"""
Get Vector Store Info Use Case
ë²¡í„°ìŠ¤í† ì–´ ì •ë³´ ì¡°íšŒ ìœ ìŠ¤ì¼€ì´ìŠ¤

ë²¡í„°ìŠ¤í† ì–´ì˜ ìƒì„¸ ì •ë³´ì™€ ì €ì¥ëœ ë°ì´í„°ì˜ í†µê³„ë¥¼ ì œê³µí•˜ëŠ” Use Caseì…ë‹ˆë‹¤.
ê³µí†µ ì˜¤ë¥˜ ì²˜ë¦¬ì™€ ì‘ë‹µ í˜•ì‹ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
"""

import logging
from typing import Dict, Any
from application.common import (
    handle_usecase_errors,
    ResponseFormatter,
    log_usecase_execution
)

logger = logging.getLogger(__name__)


class GetVectorStoreInfoUseCase:
    """ë²¡í„°ìŠ¤í† ì–´ ì •ë³´ ì¡°íšŒ ìœ ìŠ¤ì¼€ì´ìŠ¤"""
    
    def __init__(self):
        logger.info("âœ… GetVectorStoreInfoUseCase initialized")
    
    @handle_usecase_errors(
        default_error_message="ë²¡í„°ìŠ¤í† ì–´ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        log_error=True
    )
    @log_usecase_execution("GetVectorStoreInfoUseCase")
    def execute(self) -> Dict[str, Any]:
        """ë²¡í„°ìŠ¤í† ì–´ ì •ë³´ ì¡°íšŒ ì‹¤í–‰"""
        # ë²¡í„°ìŠ¤í† ì–´ ê¸°ë³¸ ì •ë³´ ì¡°íšŒ
        vector_store_info = self.embedding_service.get_vector_store_info()
        
        # ì„ë² ë”© í†µê³„ ì¡°íšŒ
        embedding_stats = self.embedding_service.get_embedding_statistics()
        
        # ì²­í¬ í†µê³„ ì¡°íšŒ
        chunk_stats = self.chunking_service.get_chunking_statistics()
        
        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„¸ ì •ë³´ êµ¬ì„± - ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©
        info = {
            "store_basic_info": {
                "store_name": vector_store_info.get("store_name", "unknown"),
                "store_type": vector_store_info.get("store_type", "unknown"),
                "initialization_status": "âœ… ì´ˆê¸°í™”ë¨" if embedding_stats.get("model_loaded", False) else "âŒ ë¯¸ì´ˆê¸°í™”",
                "search_algorithm": "ì½”ì‚¬ì¸ ìœ ì‚¬ë„",
                "storage_method": "ë©”ëª¨ë¦¬ ë‚´ ì €ì¥",
                "environment": "ë°ëª¨ í™˜ê²½"
            },
            "embedding_model_info": {
                "model_name": embedding_stats.get("model_name", "unknown"),
                "vector_dimension": embedding_stats.get("vector_dimension", 0),
                "model_type": embedding_stats.get("model_type", "unknown"),
                "sample_vector_size": f"{embedding_stats.get('vector_dimension', 0)}ì°¨ì›"
            },
            "stored_data_statistics": {
                "total_documents": chunk_stats.get("total_documents", 0),
                "total_chunks": chunk_stats.get("total_chunks", 0),
                "total_vectors": embedding_stats.get("total_embeddings", 0),
                "average_document_length": chunk_stats.get("average_chunk_length", 0.0),
                "store_size_mb": vector_store_info.get("store_size_mb", 0.0),
                "index_status": vector_store_info.get("index_status", "unknown")
            },
            "performance_info": {
                "average_embedding_time_ms": embedding_stats.get("average_embedding_time_ms", 0.0),
                "total_processing_time_ms": embedding_stats.get("total_processing_time_ms", 0.0),
                "success_rate": embedding_stats.get("success_rate", 0.0),
                "last_updated": vector_store_info.get("last_updated", "unknown")
            }
        }
        
        logger.info("âœ… ë²¡í„°ìŠ¤í† ì–´ ì •ë³´ ì¡°íšŒ ì™„ë£Œ")
        
        return ResponseFormatter.statistics_response(
            data={"vector_store_info": info},
            message="ğŸ—„ï¸ ë²¡í„°ìŠ¤í† ì–´ ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤"
        )
