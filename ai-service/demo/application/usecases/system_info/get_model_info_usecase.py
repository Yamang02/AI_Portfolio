"""
Get Model Info Use Case - Demo Application Layer
ëª¨ë¸ ì •ë³´ ì¡°íšŒ ìœ ìŠ¤ì¼€ì´ìŠ¤

ì„ë² ë”© ëª¨ë¸ê³¼ ì™¸ë¶€ LLM ëª¨ë¸ì˜ ìƒíƒœ ë° ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ì—¬ í•˜ë“œì½”ë”©ëœ ê°’ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.
ê³µí†µ ì˜¤ë¥˜ ì²˜ë¦¬ì™€ ì‘ë‹µ í˜•ì‹ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
"""

import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from datetime import datetime
import requests
import time
from application.common import (
    handle_usecase_errors,
    ResponseFormatter,
    log_usecase_execution
)
from config.demo_config_manager import get_demo_config_manager

logger = logging.getLogger(__name__)


@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´"""
    name: str
    type: str  # embedding, llm
    status: str  # loaded, unloaded, error
    version: Optional[str] = None
    parameters: Optional[Dict[str, Any]] = None
    performance_metrics: Optional[Dict[str, Any]] = None


@dataclass
class APIStatus:
    """ì™¸ë¶€ API ìƒíƒœ"""
    name: str
    endpoint: str
    status: str  # online, offline, error
    response_time_ms: Optional[float] = None
    last_checked: Optional[datetime] = None
    error_message: Optional[str] = None


class GetModelInfoUseCase:
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ ìœ ìŠ¤ì¼€ì´ìŠ¤"""
    
    def __init__(
        self,
        embedding_service=None,
        generation_service=None
    ):
        self.embedding_service = embedding_service
        self.generation_service = generation_service
        
        # DemoConfigManager ì´ˆê¸°í™”
        try:
            self.demo_config = get_demo_config_manager()
            logger.info("âœ… GetModelInfoUseCase initialized with DemoConfigManager")
        except Exception as e:
            logger.error(f"âŒ DemoConfigManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise RuntimeError("Demo ì„¤ì •ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    @handle_usecase_errors(
        default_error_message="ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        log_error=True
    )
    @log_usecase_execution("GetModelInfoUseCase")
    def execute(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤í–‰"""
        return ResponseFormatter.statistics_response(
            data={
                "embedding_models": self._get_embedding_model_info(),
                "llm_models": self._get_llm_model_info(),
                "api_status": self._get_api_status(),
                "model_performance": self._get_model_performance(),
                "resource_usage": self._get_model_resource_usage(),
                "recommendations": self._get_model_recommendations()
            },
            message="ğŸ¤– ëª¨ë¸ ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤"
        )
    
    def _get_embedding_model_info(self) -> Dict[str, Any]:
        """ì„ë² ë”© ëª¨ë¸ ì •ë³´ ì¡°íšŒ (ì„¤ì • ê¸°ë°˜)"""
        # DemoConfigManagerì—ì„œ ëª¨ë¸ ì •ë³´ ë¡œë“œ
        embedding_config = self.demo_config.get_embedding_config()
        
        # ì„œë¹„ìŠ¤ í†µê³„ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        stats = {}
        if self.embedding_service:
            try:
                stats = self.embedding_service.get_embedding_statistics()
            except Exception as e:
                logger.warning(f"ì„ë² ë”© ì„œë¹„ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return {
            "status": "loaded",
            "primary_model": {
                "name": embedding_config.get("model_name", "sentence-transformers/all-MiniLM-L6-v2"),
                "dimension": embedding_config.get("dimension", 384),
                "type": embedding_config.get("provider", "sentence-transformers"),
                "device": embedding_config.get("device", "cpu"),
                "normalize": embedding_config.get("normalize", True),
                "batch_size": embedding_config.get("batch_size", 32),
                "loaded_at": datetime.now().isoformat(),
                "memory_usage_mb": self._estimate_embedding_model_memory(embedding_config.get("dimension", 384))
            },
            "statistics": {
                "total_embeddings_created": stats.get("total_embeddings", 0),
                "vector_store_size_bytes": stats.get("total_vector_size_bytes", 0),
                "embeddings_in_memory": stats.get("vector_store_embeddings", 0),
                "average_vector_size": self._calculate_average_vector_size(stats)
            },
            "configuration": {
                "model_path": embedding_config.get("model_name", "sentence-transformers/all-MiniLM-L6-v2"),
                "max_sequence_length": 512,  # sentence-transformers ê¸°ë³¸ê°’
                "normalize_embeddings": embedding_config.get("normalize", True),
                "batch_size": embedding_config.get("batch_size", 32)
            },
            "capabilities": [
                "í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±",
                "ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°", 
                "ë°°ì¹˜ ì²˜ë¦¬",
                "ë‹¤êµ­ì–´ ì§€ì›"
            ]
        }
    
    def _get_llm_model_info(self) -> Dict[str, Any]:
        """LLM ëª¨ë¸ ì •ë³´ ì¡°íšŒ (ì„¤ì • ê¸°ë°˜)"""
        # DemoConfigManagerì—ì„œ LLM ëª¨ë¸ ì •ë³´ ë¡œë“œ
        llm_config = self.demo_config.get_llm_config()
        
        return {
            "status": "active",
            "external_apis": [
                llm_config.get("openai", {}),
                llm_config.get("google", {})
            ],
            "mock_model": llm_config.get("mock", {}),
            "local_models": [],  # í˜„ì¬ëŠ” ì™¸ë¶€ APIë§Œ ì‚¬ìš©
            "fallback_strategy": llm_config.get("fallback_strategy", "mock"),
            "response_caching": llm_config.get("response_caching", False),
            "content_filtering": llm_config.get("content_filtering", True)
        }
    
    def _get_api_status(self) -> List[Dict[str, Any]]:
        """ì™¸ë¶€ API ìƒíƒœ í™•ì¸ (ì„¤ì • ê¸°ë°˜)"""
        # DemoConfigManagerì—ì„œ API ì—”ë“œí¬ì¸íŠ¸ ì •ë³´ ë¡œë“œ
        api_endpoints = self.demo_config.get_api_endpoints()
        
        api_statuses = []
        
        # ê° APIë³„ ìƒíƒœ í™•ì¸
        for api_name, endpoint_info in api_endpoints.items():
            api_statuses.append({
                "name": f"{api_name.upper()} API",
                "endpoint": endpoint_info.get("base_url", "unknown"),
                "status": endpoint_info.get("status", "unknown"),
                "response_time_ms": None,
                "last_checked": datetime.now(),
                "note": f"API í‚¤ ì„¤ì • ì—¬ë¶€ì— ë”°ë¼ ì‹¤ì œ ìƒíƒœ í™•ì¸ ê°€ëŠ¥"
            })
        
        # Hugging Face (ì„ë² ë”© ëª¨ë¸ìš©)
        api_statuses.append({
            "name": "Hugging Face Hub",
            "endpoint": "https://huggingface.co",
            "status": "available",
            "response_time_ms": None,
            "last_checked": datetime.now(),
            "note": "ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œìš©"
        })
        
        return api_statuses
    
    def _get_model_performance(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
        performance = {
            "embedding_model": {
                "average_encoding_time_ms": 150,  # Mock ë°ì´í„°
                "tokens_per_second": 2000,
                "batch_processing_efficiency": 85.5,
                "memory_efficiency": "Good"
            },
            "llm_models": {
                "average_response_time_ms": 1200,
                "tokens_per_second": 45,
                "success_rate_percent": 98.5,
                "cache_hit_rate_percent": 0  # í˜„ì¬ ìºì‹± ë¯¸êµ¬í˜„
            },
            "overall": {
                "system_latency_ms": 1350,
                "throughput_requests_per_minute": 45,
                "error_rate_percent": 1.5
            }
        }
        
        return performance
    
    def _get_model_resource_usage(self) -> Dict[str, Any]:
        """ëª¨ë¸ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰"""
        return {
            "embedding_model": {
                "memory_usage_mb": self._estimate_embedding_model_memory(384),
                "cpu_usage_percent": 15,  # Mock ë°ì´í„°
                "gpu_usage_percent": 0,   # CPU ì „ìš©
                "disk_cache_mb": 450
            },
            "llm_models": {
                "memory_usage_mb": 0,     # ì™¸ë¶€ API ì‚¬ìš©
                "network_usage_mb": 12.5, # API í˜¸ì¶œ ë°ì´í„°
                "api_quota_usage": {
                    "openai": "unknown",
                    "anthropic": "unknown"
                }
            },
            "total": {
                "estimated_memory_mb": 450,
                "estimated_storage_mb": 1200
            }
        }
    
    def _get_model_recommendations(self) -> List[Dict[str, str]]:
        """ëª¨ë¸ ì‚¬ìš© ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        # ì„ë² ë”© ëª¨ë¸ ê¶Œì¥ì‚¬í•­
        if self.embedding_service:
            try:
                stats = self.embedding_service.get_embedding_statistics()
                if stats.get("total_embeddings", 0) > 10000:
                    recommendations.append({
                        "type": "performance",
                        "message": "ëŒ€ëŸ‰ì˜ ì„ë² ë”©ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ë²¡í„° DB ìµœì í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.",
                        "action": "ë²¡í„° ì¸ë±ì‹± ë˜ëŠ” ì••ì¶• ì ìš©"
                    })
            except:
                pass
        
        # LLM ëª¨ë¸ ê¶Œì¥ì‚¬í•­
        recommendations.extend([
            {
                "type": "cost",
                "message": "ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•´ ì‘ë‹µ ìºì‹± êµ¬í˜„ì„ ê²€í† í•´ë³´ì„¸ìš”.",
                "action": "Redis ë˜ëŠ” ë©”ëª¨ë¦¬ ìºì‹± ë„ì…"
            },
            {
                "type": "performance", 
                "message": "ë†’ì€ ì²˜ë¦¬ëŸ‰ì´ í•„ìš”í•œ ê²½ìš° ë¡œì»¬ LLM ëª¨ë¸ ê³ ë ¤",
                "action": "Ollama ë˜ëŠ” vLLM ë„ì… ê²€í† "
            },
            {
                "type": "reliability",
                "message": "API ì¥ì• ì— ëŒ€ë¹„í•œ Fallback ì „ëµ ê°•í™” í•„ìš”",
                "action": "ë‹¤ì¤‘ API ì§€ì› ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„"
            }
        ])
        
        return recommendations
    
    def _estimate_embedding_model_memory(self, dimension: int) -> float:
        """ì„ë² ë”© ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB)"""
        # sentence-transformers/all-MiniLM-L6-v2 ê¸°ì¤€ ì¶”ì •
        base_model_size = 90  # MB
        vector_storage_per_1k = dimension * 4 * 1000 / (1024 * 1024)  # float32 ê¸°ì¤€
        
        if self.embedding_service:
            try:
                stats = self.embedding_service.get_embedding_statistics()
                embeddings_count = stats.get("total_embeddings", 0)
                vector_storage = embeddings_count * dimension * 4 / (1024 * 1024)
                return base_model_size + vector_storage
            except:
                pass
        
        return base_model_size
    
    def _calculate_average_vector_size(self, stats: Dict[str, Any]) -> float:
        """í‰ê·  ë²¡í„° í¬ê¸° ê³„ì‚°"""
        total_bytes = stats.get("total_vector_size_bytes", 0)
        total_embeddings = stats.get("total_embeddings", 0)
        
        if total_embeddings > 0:
            return total_bytes / total_embeddings
        return 0.0
    
    def check_model_health(self) -> Dict[str, str]:
        """ëª¨ë¸ ê±´ê°• ìƒíƒœ ë¹ ë¥¸ í™•ì¸"""
        health = {
            "embedding_model": "unknown",
            "llm_models": "unknown",
            "overall": "unknown"
        }
        
        try:
            if self.embedding_service:
                self.embedding_service.get_embedding_statistics()
                health["embedding_model"] = "healthy"
        except:
            health["embedding_model"] = "error"
        
        try:
            if self.generation_service:
                # ê°„ë‹¨í•œ ìƒíƒœ í™•ì¸ (ì‹¤ì œë¡œëŠ” ì„œë¹„ìŠ¤ë³„ health check ë©”ì„œë“œ í•„ìš”)
                health["llm_models"] = "configured"
        except:
            health["llm_models"] = "error"
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        if health["embedding_model"] == "healthy" and health["llm_models"] in ["healthy", "configured"]:
            health["overall"] = "healthy"
        elif "error" in health.values():
            health["overall"] = "degraded" 
        else:
            health["overall"] = "unknown"
        
        return health
