"""
Get Embedding Analysis Use Case
ì„ë² ë”© ë¶„ì„ ì •ë³´ ì¡°íšŒ ìœ ìŠ¤ì¼€ì´ìŠ¤

ì„ë² ë”© ëª¨ë¸ê³¼ ìƒì„±ëœ ì„ë² ë”©ë“¤ì˜ ë¶„ì„ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” Use Caseì…ë‹ˆë‹¤.
ê³µí†µ ì˜¤ë¥˜ ì²˜ë¦¬ì™€ ì‘ë‹µ í˜•ì‹ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
"""

import logging
from typing import Dict, Any, List
from application.common import (
    handle_usecase_errors,
    ResponseFormatter,
    log_usecase_execution
)

logger = logging.getLogger(__name__)


class GetEmbeddingAnalysisUseCase:
    """ì„ë² ë”© ë¶„ì„ ì •ë³´ ì¡°íšŒ ìœ ìŠ¤ì¼€ì´ìŠ¤"""
    
    def __init__(self):
        logger.info("âœ… GetEmbeddingAnalysisUseCase initialized")
    
    @handle_usecase_errors(
        default_error_message="ì„ë² ë”© ë¶„ì„ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        log_error=True
    )
    @log_usecase_execution("GetEmbeddingAnalysisUseCase")
    def execute(self) -> Dict[str, Any]:
        """ì„ë² ë”© ë¶„ì„ ì •ë³´ ì¡°íšŒ ì‹¤í–‰"""
        # ì„ë² ë”© ì„œë¹„ìŠ¤ì—ì„œ í†µê³„ ì •ë³´ ì¡°íšŒ
        embedding_stats = self.embedding_service.get_embedding_statistics()
        
        # ì²­í‚¹ ì„œë¹„ìŠ¤ì—ì„œ ì²­í¬ í†µê³„ ì¡°íšŒ
        chunk_stats = self.chunking_service.get_chunking_statistics()
        
        # ë²¡í„°ìŠ¤í† ì–´ ì •ë³´ ì¡°íšŒ
        vector_store_info = self.embedding_service.get_vector_store_info()
        
        # ë¶„ì„ ê²°ê³¼ êµ¬ì„± - ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©
        analysis = {
            "model_info": {
                "model_name": embedding_stats.get("model_name", "unknown"),
                "vector_dimension": embedding_stats.get("vector_dimension", 0),
                "model_type": embedding_stats.get("model_type", "unknown"),
                "language_support": embedding_stats.get("language_support", "unknown"),
                "performance": embedding_stats.get("performance", "unknown")
            },
            "embedding_statistics": {
                "total_embeddings": embedding_stats.get("total_embeddings", 0),
                "total_chunks": chunk_stats.get("total_chunks", 0),
                "total_documents": chunk_stats.get("total_documents", 0),
                "average_chunk_length": chunk_stats.get("average_chunk_length", 0),
                "vector_dimension": embedding_stats.get("vector_dimension", 0)
            },
            "vector_store_info": {
                "store_type": vector_store_info.get("store_type", "unknown"),
                "total_vectors": vector_store_info.get("total_vectors", 0),
                "store_size_mb": vector_store_info.get("store_size_mb", 0),
                "index_status": vector_store_info.get("index_status", "unknown")
            },
            "performance_metrics": {
                "average_embedding_time_ms": embedding_stats.get("average_embedding_time_ms", 0.0),
                "total_processing_time_ms": embedding_stats.get("total_processing_time_ms", 0.0),
                "success_rate": embedding_stats.get("success_rate", 0.0)
            }
        }
        
        logger.info("âœ… ì„ë² ë”© ë¶„ì„ ì •ë³´ ì¡°íšŒ ì™„ë£Œ")
        
        return ResponseFormatter.statistics_response(
            data={"analysis": analysis},
            message="ğŸ“Š ì„ë² ë”© ë¶„ì„ ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤"
        )
