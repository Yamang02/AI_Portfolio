"""
Chunk Document Use Case
ë¬¸ì„œ ì²­í‚¹ ìœ ìŠ¤ì¼€ì´ìŠ¤

TextSplitter íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” Use Caseì…ë‹ˆë‹¤.
ê³µí†µ ì˜¤ë¥˜ ì²˜ë¦¬ì™€ ì‘ë‹µ í˜•ì‹ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
"""

import logging
import re
from typing import Dict, Any, Optional, List
from domain.entities.document import Document
from domain.entities.chunk import Chunk
from domain.ports.outbound.document_repository_port import DocumentRepositoryPort
from domain.ports.outbound.chunk_repository_port import ChunkRepositoryPort
from config.demo_config_manager import get_demo_config_manager
from application.common import (
    handle_usecase_errors,
    validate_required_fields,
    ResponseFormatter,
    log_usecase_execution,
    validate_string_not_empty,
    validate_positive_integer
)

logger = logging.getLogger(__name__)


class ChunkDocumentUseCase:
    """ë¬¸ì„œ ì²­í‚¹ ìœ ìŠ¤ì¼€ì´ìŠ¤"""
    
    def __init__(self, document_repository: DocumentRepositoryPort, chunk_repository: ChunkRepositoryPort):
        self.document_repository = document_repository
        self.chunk_repository = chunk_repository
        self.config_manager = get_demo_config_manager()
        logger.info("âœ… ChunkDocumentUseCase initialized")
    
    @handle_usecase_errors(
        default_error_message="ë¬¸ì„œ ì²­í‚¹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        log_error=True
    )
    @validate_required_fields(
        document_id=validate_string_not_empty
    )
    @log_usecase_execution("ChunkDocumentUseCase")
    def execute(
        self,
        document_id: str,
        chunking_strategy: Optional[str] = None,
        custom_chunk_size: Optional[int] = None,
        custom_chunk_overlap: Optional[int] = None
    ) -> Dict[str, Any]:
        """ë¬¸ì„œ ì²­í‚¹ ì‹¤í–‰"""
        # ë¬¸ì„œ ì¡°íšŒ
        document = self.document_repository.get_document_by_id(document_id)
        
        if not document:
            return ResponseFormatter.not_found_error(
                resource_type="ë¬¸ì„œ",
                resource_id=document_id,
                suggestions=[
                    "ë¬¸ì„œ IDê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "ë¬¸ì„œ ëª©ë¡ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
                ]
            )
        
        # ì¤‘ë³µ í™•ì¸: ì´ë¯¸ ì²­í‚¹ëœ ë¬¸ì„œì¸ì§€ í™•ì¸
        existing_chunks = self.chunk_repository.get_chunks_by_document_id(document_id)
        if existing_chunks:
            logger.info(f"â­ï¸ ë¬¸ì„œ '{document.source}'ëŠ” ì´ë¯¸ ì²­í‚¹ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ {len(existing_chunks)}ê°œ ì²­í¬ ë°˜í™˜")
            chunk_summaries = [
                {
                    "chunk_id": str(chunk.chunk_id),
                    "chunk_index": chunk.chunk_index,
                    "content_length": len(chunk.content),
                    "chunk_size": chunk.chunk_size,
                    "chunk_overlap": chunk.chunk_overlap,
                    "preview": chunk.get_content_preview(100)
                }
                for chunk in existing_chunks
            ]
            return ResponseFormatter.success(
                data={
                    "document_id": document_id,
                    "document_source": document.source,
                    "chunks_created": len(existing_chunks),
                    "chunks": chunk_summaries,
                    "is_cached": True
                },
                message=f"â­ï¸ ì´ë¯¸ ì²­í‚¹ëœ ë¬¸ì„œì…ë‹ˆë‹¤: {len(existing_chunks)}ê°œ ì²­í¬ ë°˜í™˜"
            )
        
        # ë¬¸ì„œ ì²­í‚¹
        chunks = self._chunk_document(
            document=document,
            chunking_strategy=chunking_strategy,
            custom_chunk_size=custom_chunk_size,
            custom_chunk_overlap=custom_chunk_overlap
        )
        
        # ì²­í¬ ì €ì¥
        self.chunk_repository.save_chunks(chunks)
        
        logger.info(f"âœ… ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {document.source} â†’ {len(chunks)}ê°œ ì²­í¬")
        
        chunk_summaries = [
            {
                "chunk_id": str(chunk.chunk_id),
                "chunk_index": chunk.chunk_index,
                "content_length": len(chunk.content),
                "chunk_size": chunk.chunk_size,
                "chunk_overlap": chunk.chunk_overlap,
                "preview": chunk.get_content_preview(100)
            }
            for chunk in chunks
        ]
        
        return ResponseFormatter.success(
            data={
                "document_id": document_id,
                "document_source": document.source,
                "chunks_created": len(chunks),
                "chunks": chunk_summaries
            },
            message=f"âœ‚ï¸ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì²­í‚¹ë˜ì—ˆìŠµë‹ˆë‹¤: {len(chunks)}ê°œ ì²­í¬ ìƒì„±"
        )
    
    def _chunk_document(
        self,
        document: Document,
        chunking_strategy: Optional[str] = None,
        custom_chunk_size: Optional[int] = None,
        custom_chunk_overlap: Optional[int] = None
    ) -> List[Chunk]:
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í•  - ê³ ê¸‰ ì „ëµ ê¸°ëŠ¥ í¬í•¨"""
        try:
            # ì„±ëŠ¥ ì„¤ì • ì ìš©
            self._apply_performance_settings()
            
            # ë¬¸ì„œ ìœ í˜• ìë™ ê°ì§€
            if not chunking_strategy:
                chunking_strategy = self._detect_document_type(document)
            
            # ì„¤ì •ì—ì„œ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
            chunking_config = self.config_manager.get_chunking_config()
            strategy_config = chunking_config.get("chunking_strategies", {}).get(chunking_strategy, {})
            params = strategy_config.get("parameters", {})
            
            # ConfigManagerì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°
            demo_config = self.config_manager.get_demo_config()
            rag_config = demo_config.get("rag", {})
            default_chunk_size = rag_config.get("chunk_size", 500)
            default_chunk_overlap = rag_config.get("chunk_overlap", 75)
            
            # ì „ëµë³„ ê¸°ë³¸ê°’ ìš°ì„  ì‚¬ìš©, ìˆ˜ë™ ì„¤ì •ì€ ì˜¤ë²„ë¼ì´ë“œë¡œ ì‚¬ìš©
            chunk_size = params.get("chunk_size", default_chunk_size)
            chunk_overlap = params.get("chunk_overlap", default_chunk_overlap)
            
            # ìˆ˜ë™ ì„¤ì •ì´ ì œê³µëœ ê²½ìš°ì—ë§Œ ì˜¤ë²„ë¼ì´ë“œ
            if custom_chunk_size is not None:
                chunk_size = custom_chunk_size
                logger.info(f"ğŸ”§ ìˆ˜ë™ ì²­í¬ í¬ê¸° ì˜¤ë²„ë¼ì´ë“œ: {chunk_size}")
            if custom_chunk_overlap is not None:
                chunk_overlap = custom_chunk_overlap
                logger.info(f"ğŸ”§ ìˆ˜ë™ ì²­í¬ ê²¹ì¹¨ ì˜¤ë²„ë¼ì´ë“œ: {chunk_overlap}")
            
            preserve_structure = params.get("preserve_structure", True)
            
            # ë””ë²„ê¹… ë¡œê·¸ ì¶œë ¥
            self._log_strategy_selection(chunking_strategy, strategy_config)
            
            logger.info(f"ğŸ“‹ ì²­í‚¹ ì „ëµ: {chunking_strategy} (í¬ê¸°: {chunk_size}, ê²¹ì¹¨: {chunk_overlap})")
            
            # ì „ëµë³„ ì²­í‚¹ ì‹¤í–‰
            if chunking_strategy == "PROJECT":
                chunks = self._chunk_project_document(document, chunk_size, chunk_overlap, strategy_config)
            elif chunking_strategy == "QA":
                chunks = self._chunk_qa_document(document, chunk_size, chunk_overlap, strategy_config)
            else:
                chunks = self._chunk_text_document(document, chunk_size, chunk_overlap, preserve_structure)
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            chunks = self._add_keyword_metadata(chunks, strategy_config)
            
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²­í¬ ì •ë ¬
            chunks = self._apply_priority_sorting(chunks, strategy_config)
            
            # ì²­í¬ ë©”íƒ€ë°ì´í„° ë¡œê¹…
            self._log_chunk_metadata(chunks)
            
            logger.info(f"âœ… ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {document.source} â†’ {len(chunks)}ê°œ ì²­í¬ ({chunking_strategy} ì „ëµ)")
            return chunks
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²­í‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def _detect_document_type(self, document: Document) -> str:
        """ë¬¸ì„œ ìœ í˜• ìë™ ê°ì§€"""
        # ë¨¼ì € Document ë©”íƒ€ë°ì´í„°ì—ì„œ document_type í™•ì¸
        if document.metadata and document.metadata.document_type:
            doc_type = document.metadata.document_type.value
            logger.info(f"ğŸ“‹ ë©”íƒ€ë°ì´í„°ì—ì„œ ë¬¸ì„œ ìœ í˜• ê°ì§€: {doc_type}")
            return doc_type
        
        # ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë‚´ìš© ê¸°ë°˜ ê°ì§€
        chunking_config = self.config_manager.get_chunking_config()
        detection_config = chunking_config.get("document_detection", {})
        content = document.content.lower()
        source = document.source.lower()
        
        # íŒŒì¼ ê²½ë¡œ/ì´ë¦„ ê¸°ë°˜ ê°ì§€
        path_patterns = detection_config.get("path_patterns", {})
        for doc_type, patterns in path_patterns.items():
            for pattern in patterns:
                if re.search(pattern, source):
                    logger.info(f"ğŸ“‹ ê²½ë¡œ íŒ¨í„´ìœ¼ë¡œ {doc_type} ê°ì§€: {pattern}")
                    return doc_type
        
        # ë‚´ìš© ê¸°ë°˜ ê°ì§€
        content_patterns = detection_config.get("content_patterns", {})
        for doc_type, config in content_patterns.items():
            patterns = config.get("patterns", [])
            min_matches = config.get("min_matches", 1)
            matches = 0
            
            for pattern in patterns:
                if re.search(pattern, content):
                    matches += 1
            
            if matches >= min_matches:
                logger.info(f"ğŸ“‹ ë‚´ìš© íŒ¨í„´ìœ¼ë¡œ {doc_type} ê°ì§€: {matches}ê°œ ë§¤ì¹­")
                return doc_type
        
        # ê¸°ë³¸ê°’
        logger.info("ğŸ“‹ ê¸°ë³¸ TEXT ì „ëµ ì‚¬ìš©")
        return "TEXT"
    
    def _chunk_project_document(
        self,
        document: Document,
        chunk_size: int,
        chunk_overlap: int,
        strategy_config: Dict[str, Any]
    ) -> List[Chunk]:
        """í”„ë¡œì íŠ¸ ë¬¸ì„œ íŠ¹í™” ì²­í‚¹ - ê³ ê¸‰ ê¸°ëŠ¥ í¬í•¨"""
        content = document.content
        chunks = []
        chunk_index = 0
        
        # íŠ¹ë³„ ì²˜ë¦¬ ì„¤ì • í™•ì¸
        special_processing = strategy_config.get("special_processing", {})
        
        # Frontmatter ì¶”ì¶œ
        if special_processing.get("frontmatter_extraction", False):
            frontmatter, main_content = self._extract_frontmatter(content)
            if frontmatter:
                # Frontmatterë¥¼ ë³„ë„ ì²­í¬ë¡œ ìƒì„±
                frontmatter_chunk = self._create_chunk(
                    content=f"Frontmatter:\n{frontmatter}",
                    document_id=document.document_id,
                    chunk_index=chunk_index,
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
                chunks.append(frontmatter_chunk)
                chunk_index += 1
                content = main_content  # ë©”ì¸ ì½˜í…ì¸ ë¡œ ì—…ë°ì´íŠ¸
        
        # ì„¹ì…˜ ìš°ì„ ìˆœìœ„ ê°€ì ¸ì˜¤ê¸°
        section_priorities = strategy_config.get("section_priorities", {})
        
        # ì„¹ì…˜ë³„ ë¶„í• 
        sections = self._split_into_sections(content)
        
        for section_name, section_content in sections:
            priority = section_priorities.get(section_name, 999)
            
            # ì„¹ì…˜ë³„ íŠ¹ë³„ ì²˜ë¦¬
            if section_name == "Timeline" and special_processing.get("timeline_section", False):
                timeline_chunks = self._chunk_timeline_section(section_content, document.document_id, chunk_index, chunk_size, chunk_overlap)
                chunks.extend(timeline_chunks)
                chunk_index += len(timeline_chunks)
            else:
                # ì¼ë°˜ ì„¹ì…˜ ì²­í‚¹
                section_chunks = self._chunk_by_sentences(section_content, document.document_id, chunk_index, chunk_size, chunk_overlap)
                chunks.extend(section_chunks)
                chunk_index += len(section_chunks)
        
        logger.info(f"ğŸ“‹ í”„ë¡œì íŠ¸ ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ (ì„¹ì…˜: {len(sections)}ê°œ)")
        return chunks
    
    def _chunk_qa_document(
        self,
        document: Document,
        chunk_size: int,
        chunk_overlap: int,
        strategy_config: Dict[str, Any]
    ) -> List[Chunk]:
        """Q&A ë¬¸ì„œ íŠ¹í™” ì²­í‚¹ - ê³ ê¸‰ ê¸°ëŠ¥ í¬í•¨"""
        content = document.content
        chunks = []
        chunk_index = 0
        
        # íŠ¹ë³„ ì²˜ë¦¬ ì„¤ì • í™•ì¸
        special_processing = strategy_config.get("special_processing", {})
        
        # ë¬¸ì„œ ê°œìš” ì„¹ì…˜ ì¶”ì¶œ
        if special_processing.get("intro_extraction", False):
            intro_content, main_content = self._extract_intro_section(content)
            if intro_content:
                # ê°œìš”ë¥¼ ë³„ë„ ì²­í¬ë¡œ ìƒì„±
                intro_chunk = self._create_chunk(
                    content=f"ë¬¸ì„œ ê°œìš”:\n{intro_content}",
                    document_id=document.document_id,
                    chunk_index=chunk_index,
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
                chunks.append(intro_chunk)
                chunk_index += 1
                content = main_content  # ë©”ì¸ ì½˜í…ì¸ ë¡œ ì—…ë°ì´íŠ¸
        
        # ì¹´í…Œê³ ë¦¬ ê°ì§€ ë° ìš°ì„ ìˆœìœ„ ì ìš©
        category_priorities = strategy_config.get("category_priorities", {})
        detected_category = self._detect_category_from_content(content, category_priorities)
        
        # Q&A íŒ¨í„´ ì²˜ë¦¬
        processing_patterns = strategy_config.get("processing_patterns", {})
        qa_patterns = processing_patterns.get("qa_patterns", [])
        
        if qa_patterns:
            # íŒ¨í„´ ê¸°ë°˜ Q&A ì¶”ì¶œ
            qa_pairs = self._extract_qa_pairs(content, qa_patterns)
            
            for qa_pair in qa_pairs:
                if len(qa_pair) <= chunk_size:
                    # Q&A ìŒì´ ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
                    chunk = self._create_chunk(
                        content=qa_pair,
                        document_id=document.document_id,
                        chunk_index=chunk_index,
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap
                    )
                    chunks.append(chunk)
                    chunk_index += 1
                else:
                    # í° Q&AëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                    section_chunks = self._chunk_by_sentences(qa_pair, document.document_id, chunk_index, chunk_size, chunk_overlap)
                    chunks.extend(section_chunks)
                    chunk_index += len(section_chunks)
        else:
            # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ë¬¸ì¥ ë‹¨ìœ„ ì²­í‚¹
            chunks.extend(self._chunk_by_sentences(content, document.document_id, chunk_index, chunk_size, chunk_overlap))
        
        logger.info(f"ğŸ“‹ Q&A ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ (ì¹´í…Œê³ ë¦¬: {detected_category})")
        return chunks
    
    def _chunk_text_document(
        self,
        document: Document,
        chunk_size: int,
        chunk_overlap: int,
        preserve_structure: bool
    ) -> List[Chunk]:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ ë¬¸ì„œ ì²­í‚¹"""
        content = document.content
        
        if preserve_structure:
            # êµ¬ì¡° ë³´ì¡´ ì²­í‚¹ (ë‹¨ë½ ìš°ì„ , ë¬¸ì¥ ë‹¨ìœ„)
            return self._chunk_by_paragraphs(content, document.document_id, 0, chunk_size, chunk_overlap)
        else:
            # ë‹¨ìˆœ ë¬¸ì¥ ë‹¨ìœ„ ì²­í‚¹
            return self._chunk_by_sentences(content, document.document_id, 0, chunk_size, chunk_overlap)
    
    def _split_into_sections(self, content: str) -> List[tuple[str, str]]:
        """ë¬¸ì„œë¥¼ ì„¹ì…˜ë³„ë¡œ ë¶„í• """
        sections = []
        lines = content.split('\n')
        current_section = ""
        current_section_name = "ê¸°ë³¸"
        
        for line in lines:
            # ì„¹ì…˜ í—¤ë” ê°ì§€ (## ë˜ëŠ” ###)
            if line.startswith('## '):
                if current_section.strip():
                    sections.append((current_section_name, current_section.strip()))
                current_section_name = line[3:].strip()
                current_section = line + '\n'
            elif line.startswith('### '):
                if current_section.strip():
                    sections.append((current_section_name, current_section.strip()))
                current_section_name = line[4:].strip()
                current_section = line + '\n'
            else:
                current_section += line + '\n'
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì¶”ê°€
        if current_section.strip():
            sections.append((current_section_name, current_section.strip()))
        
        return sections
    
    def _extract_qa_pairs(self, content: str, patterns: List[str]) -> List[str]:
        """Q&A ìŒ ì¶”ì¶œ"""
        qa_pairs = []
        
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.DOTALL)
            for match in matches:
                qa_pair = match.group(0)
                qa_pairs.append(qa_pair)
        
        return qa_pairs
    
    def _chunk_timeline_section(
        self,
        timeline_content: str,
        document_id: str,
        chunk_index: int,
        chunk_size: int,
        chunk_overlap: int
    ) -> List[Chunk]:
        """Timeline ì„¹ì…˜ íŠ¹í™” ì²­í‚¹"""
        chunks = []
        lines = timeline_content.split('\n')
        current_chunk = ""
        
        for line in lines:
            # ì—°ë„/ë‚ ì§œ íŒ¨í„´ ê°ì§€
            if re.match(r'^\d{4}', line) or re.match(r'^\*\s*\d{4}', line):
                if current_chunk.strip():
                    chunk = self._create_chunk(
                        content=current_chunk.strip(),
                        document_id=document_id,
                        chunk_index=chunk_index,
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap
                    )
                    chunks.append(chunk)
                    chunk_index += 1
                    current_chunk = ""
            
            current_chunk += line + '\n'
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            chunk = self._create_chunk(
                content=current_chunk.strip(),
                document_id=document_id,
                chunk_index=chunk_index,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            chunks.append(chunk)
        
        return chunks
    
    def _chunk_by_sentences(
        self,
        content: str,
        document_id: str,
        chunk_index: int,
        chunk_size: int,
        chunk_overlap: int
    ) -> List[Chunk]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í‚¹"""
        sentences = self._split_into_sentences(content)
        chunks = []
        current_chunk = ""
        current_index = chunk_index
        
        for sentence in sentences:
            if len(current_chunk + sentence) <= chunk_size:
                current_chunk += sentence
            else:
                if current_chunk.strip():
                    chunk = self._create_chunk(
                        content=current_chunk.strip(),
                        document_id=document_id,
                        chunk_index=current_index,
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap
                    )
                    chunks.append(chunk)
                    current_index += 1
                
                current_chunk = sentence
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            chunk = self._create_chunk(
                content=current_chunk.strip(),
                document_id=document_id,
                chunk_index=current_index,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            chunks.append(chunk)
        
        return chunks
    
    def _chunk_by_paragraphs(
        self,
        content: str,
        document_id: str,
        chunk_index: int,
        chunk_size: int,
        chunk_overlap: int
    ) -> List[Chunk]:
        """ë‹¨ë½ ë‹¨ìœ„ë¡œ ì²­í‚¹"""
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = ""
        current_index = chunk_index
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
                
            if len(current_chunk + paragraph) <= chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk.strip():
                    chunk = self._create_chunk(
                        content=current_chunk.strip(),
                        document_id=document_id,
                        chunk_index=current_index,
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap
                    )
                    chunks.append(chunk)
                    current_index += 1
                
                current_chunk = paragraph + "\n\n"
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            chunk = self._create_chunk(
                content=current_chunk.strip(),
                document_id=document_id,
                chunk_index=current_index,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            chunks.append(chunk)
        
        return chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        # ë” ì •êµí•œ ë¬¸ì¥ ë¶„í•  (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ê¸°ì¤€, ë‹¨ ì•½ì–´ëŠ” ì œì™¸)
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _create_chunk(
        self,
        content: str,
        document_id: str,
        chunk_index: int,
        chunk_size: int,
        chunk_overlap: int
    ) -> Chunk:
        """ì²­í¬ ìƒì„±"""
        return Chunk(
            content=content,
            document_id=document_id,
            chunk_index=chunk_index,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
    
    def _apply_performance_settings(self):
        """ì„±ëŠ¥ ì„¤ì • ì ìš©"""
        try:
            chunking_config = self.config_manager.get_chunking_config()
            performance_config = chunking_config.get("performance", {})
            
            max_document_size = performance_config.get("max_document_size", 1000000)
            cache_compiled_patterns = performance_config.get("cache_compiled_patterns", True)
            parallel_processing = performance_config.get("parallel_processing", False)
            
            logger.info(f"âš¡ ì„±ëŠ¥ ì„¤ì • ì ìš©: ìµœëŒ€ë¬¸ì„œí¬ê¸°={max_document_size}, íŒ¨í„´ìºì‹œ={cache_compiled_patterns}, ë³‘ë ¬ì²˜ë¦¬={parallel_processing}")
            
            # ì»´íŒŒì¼ëœ íŒ¨í„´ ìºì‹œ (í–¥í›„ êµ¬í˜„)
            if cache_compiled_patterns:
                self._compiled_patterns_cache = {}
            
        except Exception as e:
            logger.warning(f"ì„±ëŠ¥ ì„¤ì • ì ìš© ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _log_strategy_selection(self, strategy_name: str, strategy_config: Dict[str, Any]):
        """ì „ëµ ì„ íƒ ë¡œê¹…"""
        try:
            chunking_config = self.config_manager.get_chunking_config()
            debug_config = chunking_config.get("debug", {})
            
            if debug_config.get("log_strategy_selection", True):
                logger.info(f"ğŸ¯ ì „ëµ ì„ íƒ: {strategy_name}")
                logger.info(f"ğŸ“‹ ì „ëµ ì„¤ì •: {strategy_config.get('name', 'Unknown')}")
                logger.info(f"ğŸ“ ì „ëµ ì„¤ëª…: {strategy_config.get('description', 'No description')}")
                
                # íŒŒë¼ë¯¸í„° ë¡œê¹…
                params = strategy_config.get("parameters", {})
                logger.info(f"âš™ï¸ ì „ëµ íŒŒë¼ë¯¸í„°: {params}")
                
        except Exception as e:
            logger.warning(f"ì „ëµ ì„ íƒ ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _add_keyword_metadata(self, chunks: List[Chunk], strategy_config: Dict[str, Any]) -> List[Chunk]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        try:
            keywords_config = strategy_config.get("keywords", {})
            tech_patterns = keywords_config.get("tech_patterns", [])
            concept_patterns = keywords_config.get("concept_patterns", [])
            
            if not tech_patterns and not concept_patterns:
                return chunks
            
            # íŒ¨í„´ ì»´íŒŒì¼
            compiled_tech_patterns = []
            compiled_concept_patterns = []
            
            for pattern in tech_patterns:
                try:
                    compiled_tech_patterns.append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    logger.warning(f"ê¸°ìˆ  íŒ¨í„´ ì»´íŒŒì¼ ì‹¤íŒ¨: {pattern} - {e}")
            
            for pattern in concept_patterns:
                try:
                    compiled_concept_patterns.append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    logger.warning(f"ê°œë… íŒ¨í„´ ì»´íŒŒì¼ ì‹¤íŒ¨: {pattern} - {e}")
            
            # ê° ì²­í¬ì— í‚¤ì›Œë“œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for chunk in chunks:
                tech_keywords = []
                concept_keywords = []
                
                # ê¸°ìˆ  í‚¤ì›Œë“œ ê²€ìƒ‰
                for pattern in compiled_tech_patterns:
                    matches = pattern.findall(chunk.content)
                    tech_keywords.extend(matches)
                
                # ê°œë… í‚¤ì›Œë“œ ê²€ìƒ‰
                for pattern in compiled_concept_patterns:
                    matches = pattern.findall(chunk.content)
                    concept_keywords.extend(matches)
                
                # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
                tech_keywords = list(set([kw.strip() for kw in tech_keywords if kw.strip()]))
                concept_keywords = list(set([kw.strip() for kw in concept_keywords if kw.strip()]))
                
                # ì²­í¬ì— í‚¤ì›Œë“œ ì •ë³´ ì¶”ê°€ (ë©”íƒ€ë°ì´í„°ë¡œ ì €ì¥)
                if hasattr(chunk, 'metadata') and chunk.metadata:
                    chunk.metadata.keywords = {
                        "tech_keywords": tech_keywords,
                        "concept_keywords": concept_keywords,
                        "keyword_count": len(tech_keywords) + len(concept_keywords)
                    }
                
                logger.debug(f"ì²­í¬ {chunk.chunk_index}: ê¸°ìˆ í‚¤ì›Œë“œ={len(tech_keywords)}, ê°œë…í‚¤ì›Œë“œ={len(concept_keywords)}")
            
            logger.info(f"ğŸ” í‚¤ì›Œë“œ ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
            return chunks
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return chunks
    
    def _apply_priority_sorting(self, chunks: List[Chunk], strategy_config: Dict[str, Any]) -> List[Chunk]:
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²­í¬ ì •ë ¬"""
        try:
            # ì„¹ì…˜ ìš°ì„ ìˆœìœ„ ê°€ì ¸ì˜¤ê¸°
            section_priorities = strategy_config.get("section_priorities", {})
            category_priorities = strategy_config.get("category_priorities", {})
            
            if not section_priorities and not category_priorities:
                return chunks
            
            # ì²­í¬ë³„ ìš°ì„ ìˆœìœ„ ê³„ì‚°
            chunks_with_priority = []
            
            for chunk in chunks:
                priority = 999  # ê¸°ë³¸ ìš°ì„ ìˆœìœ„ (ë‚®ìŒ)
                
                # ì„¹ì…˜ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
                if section_priorities:
                    # ì²­í¬ ë‚´ìš©ì—ì„œ ì„¹ì…˜ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                    content_lower = chunk.content.lower()
                    for section_name, section_priority in section_priorities.items():
                        if section_name.lower() in content_lower:
                            priority = min(priority, section_priority)
                            break
                
                # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
                if category_priorities:
                    # ë¬¸ì„œ ì†ŒìŠ¤ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                    source_lower = chunk.document_id.lower()
                    for category, category_priority in category_priorities.items():
                        if category.lower() in source_lower:
                            priority = min(priority, category_priority)
                            break
                
                chunks_with_priority.append((priority, chunk))
            
            # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬ (ë‚®ì€ ìˆ«ìê°€ ë†’ì€ ìš°ì„ ìˆœìœ„)
            chunks_with_priority.sort(key=lambda x: x[0])
            
            # ì •ë ¬ëœ ì²­í¬ ë°˜í™˜
            sorted_chunks = [chunk for _, chunk in chunks_with_priority]
            
            logger.info(f"ğŸ“Š ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì •ë ¬ ì™„ë£Œ: {len(sorted_chunks)}ê°œ ì²­í¬")
            return sorted_chunks
            
        except Exception as e:
            logger.error(f"ìš°ì„ ìˆœìœ„ ì •ë ¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return chunks
    
    def _log_chunk_metadata(self, chunks: List[Chunk]):
        """ì²­í¬ ë©”íƒ€ë°ì´í„° ë¡œê¹…"""
        try:
            chunking_config = self.config_manager.get_chunking_config()
            debug_config = chunking_config.get("debug", {})
            
            if debug_config.get("log_chunk_metadata", False):
                logger.info(f"ğŸ“Š ì²­í¬ ë©”íƒ€ë°ì´í„° ë¡œê¹…:")
                for i, chunk in enumerate(chunks):
                    logger.info(f"  ì²­í¬ {i}: í¬ê¸°={len(chunk.content)}, ì¸ë±ìŠ¤={chunk.chunk_index}")
                    
                    # í‚¤ì›Œë“œ ì •ë³´ê°€ ìˆìœ¼ë©´ ë¡œê¹…
                    if hasattr(chunk, 'metadata') and chunk.metadata and hasattr(chunk.metadata, 'keywords'):
                        keywords = chunk.metadata.keywords
                        logger.info(f"    í‚¤ì›Œë“œ: ê¸°ìˆ ={keywords.get('tech_keywords', [])}, ê°œë…={keywords.get('concept_keywords', [])}")
            
        except Exception as e:
            logger.warning(f"ì²­í¬ ë©”íƒ€ë°ì´í„° ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _extract_frontmatter(self, content: str) -> tuple[str, str]:
        """YAML frontmatter ì¶”ì¶œ"""
        try:
            # YAML frontmatter íŒ¨í„´ (---ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ”)
            frontmatter_pattern = r'^---\s*\n(.*?)\n---\s*\n(.*)$'
            match = re.match(frontmatter_pattern, content, re.DOTALL)
            
            if match:
                frontmatter = match.group(1).strip()
                main_content = match.group(2).strip()
                logger.info(f"ğŸ“„ Frontmatter ì¶”ì¶œ ì™„ë£Œ: {len(frontmatter)}ì")
                return frontmatter, main_content
            else:
                return "", content
                
        except Exception as e:
            logger.warning(f"Frontmatter ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return "", content
    
    def _extract_intro_section(self, content: str) -> tuple[str, str]:
        """ë¬¸ì„œ ê°œìš” ì„¹ì…˜ ì¶”ì¶œ"""
        try:
            # ê°œìš” ì„¹ì…˜ íŒ¨í„´ë“¤
            intro_patterns = [
                r'^#\s*ê°œìš”\s*\n(.*?)(?=\n#|\n##|\Z)',
                r'^#\s*ì†Œê°œ\s*\n(.*?)(?=\n#|\n##|\Z)',
                r'^#\s*Overview\s*\n(.*?)(?=\n#|\n##|\Z)',
                r'^##\s*ê°œìš”\s*\n(.*?)(?=\n#|\n##|\Z)',
                r'^##\s*ì†Œê°œ\s*\n(.*?)(?=\n#|\n##|\Z)'
            ]
            
            for pattern in intro_patterns:
                match = re.search(pattern, content, re.DOTALL | re.MULTILINE)
                if match:
                    intro_content = match.group(1).strip()
                    # ê°œìš” ì„¹ì…˜ì„ ì œê±°í•œ ë‚˜ë¨¸ì§€ ë‚´ìš©
                    main_content = content.replace(match.group(0), '').strip()
                    logger.info(f"ğŸ“„ ê°œìš” ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ: {len(intro_content)}ì")
                    return intro_content, main_content
            
            return "", content
                
        except Exception as e:
            logger.warning(f"ê°œìš” ì„¹ì…˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return "", content
    
    def _detect_category_from_content(self, content: str, category_priorities: Dict[str, int]) -> str:
        """ì½˜í…ì¸ ì—ì„œ ì¹´í…Œê³ ë¦¬ ê°ì§€"""
        try:
            if not category_priorities:
                return "unknown"
            
            content_lower = content.lower()
            category_scores = {}
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
            category_keywords = {
                "architecture": ["ì•„í‚¤í…ì²˜", "ì„¤ê³„", "êµ¬ì¡°", "architecture", "design"],
                "ai-services": ["ai", "ì„œë¹„ìŠ¤", "ëª¨ë¸", "llm", "ì„ë² ë”©", "rag"],
                "deployment": ["ë°°í¬", "deployment", "docker", "kubernetes", "ci/cd"],
                "development": ["ê°œë°œ", "development", "ì½”ë”©", "í”„ë¡œê·¸ë˜ë°", "êµ¬í˜„"],
                "performance": ["ì„±ëŠ¥", "performance", "ìµœì í™”", "optimization"],
                "troubleshooting": ["ë¬¸ì œ", "ì˜¤ë¥˜", "ë””ë²„ê¹…", "troubleshooting", "í•´ê²°"],
                "decisions": ["ê²°ì •", "decision", "ì„ íƒ", "ê³ ë ¤ì‚¬í•­"],
                "learning": ["í•™ìŠµ", "learning", "ë°°ìš´ì ", "ê²½í—˜"],
                "frontend": ["í”„ë¡ íŠ¸ì—”ë“œ", "frontend", "ui", "ux", "react", "vue"]
            }
            
            for category, priority in category_priorities.items():
                keywords = category_keywords.get(category, [])
                score = 0
                
                for keyword in keywords:
                    if keyword in content_lower:
                        score += 1
                
                if score > 0:
                    category_scores[category] = score * (10 - priority)  # ìš°ì„ ìˆœìœ„ ë°˜ì˜
            
            if category_scores:
                detected_category = max(category_scores, key=category_scores.get)
                logger.info(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ ê°ì§€: {detected_category} (ì ìˆ˜: {category_scores[detected_category]})")
                return detected_category
            
            return "unknown"
                
        except Exception as e:
            logger.warning(f"ì¹´í…Œê³ ë¦¬ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return "unknown"