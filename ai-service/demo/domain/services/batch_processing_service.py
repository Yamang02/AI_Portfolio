"""
Batch Processing Service - Demo Domain Layer
ë°ëª¨ ë„ë©”ì¸ ë°°ì¹˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤

ë°°ì¹˜ ì‘ì—…ì˜ ìƒì„±, ì‹¤í–‰, ëª¨ë‹ˆí„°ë§ì„ ë‹´ë‹¹í•˜ëŠ” ë„ë©”ì¸ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
"""

import logging
from typing import Dict, List, Optional, Callable, Any
from datetime import datetime
from ..entities.batch_job import BatchJob, BatchJobType, BatchJobStatus, BatchJobId
from ..entities.chunk import Chunk
from ..entities.embedding import Embedding
from ..entities.processing_status import ProcessingStage

logger = logging.getLogger(__name__)


class BatchProcessingService:
    """ë°°ì¹˜ ì²˜ë¦¬ ë„ë©”ì¸ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.batch_jobs: Dict[str, BatchJob] = {}
        logger.info("âœ… Batch Processing Service initialized")
    
    def create_embedding_batch_job(self, chunks: List[Chunk]) -> BatchJob:
        """ì„ë² ë”© ìƒì„± ë°°ì¹˜ ì‘ì—… ìƒì„±"""
        try:
            batch_job = BatchJob(
                job_type=BatchJobType.EMBEDDING_CREATION,
                total_items=len(chunks),
                metadata={
                    "chunk_ids": [str(chunk.chunk_id) for chunk in chunks],
                    "document_ids": list(set(str(chunk.document_id) for chunk in chunks))
                }
            )
            
            self.batch_jobs[str(batch_job.batch_job_id)] = batch_job
            logger.info(f"âœ… ì„ë² ë”© ìƒì„± ë°°ì¹˜ ì‘ì—… ìƒì„±: {len(chunks)}ê°œ ì²­í¬")
            return batch_job
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ë°°ì¹˜ ì‘ì—… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def create_vector_store_batch_job(self, embeddings: List[Embedding]) -> BatchJob:
        """ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ë°°ì¹˜ ì‘ì—… ìƒì„±"""
        try:
            batch_job = BatchJob(
                job_type=BatchJobType.VECTOR_STORE_SAVE,
                total_items=len(embeddings),
                metadata={
                    "embedding_ids": [str(embedding.embedding_id) for embedding in embeddings],
                    "chunk_ids": [str(embedding.chunk_id) for embedding in embeddings]
                }
            )
            
            self.batch_jobs[str(batch_job.batch_job_id)] = batch_job
            logger.info(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ë°°ì¹˜ ì‘ì—… ìƒì„±: {len(embeddings)}ê°œ ì„ë² ë”©")
            return batch_job
            
        except Exception as e:
            logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ë°°ì¹˜ ì‘ì—… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def create_full_pipeline_batch_job(self, chunks: List[Chunk]) -> BatchJob:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ë°°ì¹˜ ì‘ì—… ìƒì„±"""
        try:
            batch_job = BatchJob(
                job_type=BatchJobType.FULL_PIPELINE,
                total_items=len(chunks),
                metadata={
                    "chunk_ids": [str(chunk.chunk_id) for chunk in chunks],
                    "document_ids": list(set(str(chunk.document_id) for chunk in chunks)),
                    "pipeline_stages": ["CHUNK_LOADED", "EMBEDDING_CREATION", "VECTOR_STORE_SAVE"]
                }
            )
            
            self.batch_jobs[str(batch_job.batch_job_id)] = batch_job
            logger.info(f"âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ë°°ì¹˜ ì‘ì—… ìƒì„±: {len(chunks)}ê°œ ì²­í¬")
            return batch_job
            
        except Exception as e:
            logger.error(f"ì „ì²´ íŒŒì´í”„ë¼ì¸ ë°°ì¹˜ ì‘ì—… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def execute_batch_job(
        self, 
        batch_job_id: str, 
        processing_function: Callable,
        *args, 
        **kwargs
    ) -> BatchJob:
        """ë°°ì¹˜ ì‘ì—… ì‹¤í–‰"""
        try:
            batch_job = self.get_batch_job(batch_job_id)
            if not batch_job:
                raise ValueError(f"ë°°ì¹˜ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {batch_job_id}")
            
            if batch_job.status != BatchJobStatus.PENDING:
                raise ValueError(f"ì‹¤í–‰í•  ìˆ˜ ì—†ëŠ” ë°°ì¹˜ ì‘ì—… ìƒíƒœ: {batch_job.status.value}")
            
            batch_job.start()
            logger.info(f"ğŸš€ ë°°ì¹˜ ì‘ì—… ì‹œì‘: {batch_job.job_type.value}")
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
            try:
                result = processing_function(batch_job, *args, **kwargs)
                batch_job.complete()
                logger.info(f"âœ… ë°°ì¹˜ ì‘ì—… ì™„ë£Œ: {batch_job.job_type.value}")
                return batch_job
                
            except Exception as e:
                batch_job.fail(str(e))
                logger.error(f"âŒ ë°°ì¹˜ ì‘ì—… ì‹¤íŒ¨: {batch_job.job_type.value} - {str(e)}")
                raise
                
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def update_batch_job_progress(
        self, 
        batch_job_id: str, 
        processed: int, 
        failed: int = 0
    ) -> Optional[BatchJob]:
        """ë°°ì¹˜ ì‘ì—… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        try:
            batch_job = self.get_batch_job(batch_job_id)
            if not batch_job:
                return None
            
            batch_job.update_progress(processed, failed)
            logger.debug(f"ğŸ“Š ë°°ì¹˜ ì‘ì—… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: {batch_job.get_progress_percentage():.1f}%")
            return batch_job
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì‘ì—… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def get_batch_job(self, batch_job_id: str) -> Optional[BatchJob]:
        """ë°°ì¹˜ ì‘ì—… ì¡°íšŒ"""
        return self.batch_jobs.get(batch_job_id)
    
    def get_batch_jobs_by_type(self, job_type: BatchJobType) -> List[BatchJob]:
        """íƒ€ì…ë³„ ë°°ì¹˜ ì‘ì—… ì¡°íšŒ"""
        return [
            job for job in self.batch_jobs.values()
            if job.job_type == job_type
        ]
    
    def get_running_batch_jobs(self) -> List[BatchJob]:
        """ì‹¤í–‰ ì¤‘ì¸ ë°°ì¹˜ ì‘ì—… ì¡°íšŒ"""
        return [
            job for job in self.batch_jobs.values()
            if job.is_running()
        ]
    
    def get_completed_batch_jobs(self) -> List[BatchJob]:
        """ì™„ë£Œëœ ë°°ì¹˜ ì‘ì—… ì¡°íšŒ"""
        return [
            job for job in self.batch_jobs.values()
            if job.is_completed()
        ]
    
    def get_failed_batch_jobs(self) -> List[BatchJob]:
        """ì‹¤íŒ¨í•œ ë°°ì¹˜ ì‘ì—… ì¡°íšŒ"""
        return [
            job for job in self.batch_jobs.values()
            if job.status == BatchJobStatus.FAILED
        ]
    
    def cancel_batch_job(self, batch_job_id: str) -> Optional[BatchJob]:
        """ë°°ì¹˜ ì‘ì—… ì·¨ì†Œ"""
        try:
            batch_job = self.get_batch_job(batch_job_id)
            if not batch_job:
                return None
            
            if batch_job.is_running():
                batch_job.cancel()
                logger.info(f"ğŸ›‘ ë°°ì¹˜ ì‘ì—… ì·¨ì†Œ: {batch_job.job_type.value}")
            
            return batch_job
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì‘ì—… ì·¨ì†Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def retry_failed_batch_job(
        self, 
        batch_job_id: str, 
        processing_function: Callable,
        *args, 
        **kwargs
    ) -> Optional[BatchJob]:
        """ì‹¤íŒ¨í•œ ë°°ì¹˜ ì‘ì—… ì¬ì‹œë„"""
        try:
            batch_job = self.get_batch_job(batch_job_id)
            if not batch_job or batch_job.status != BatchJobStatus.FAILED:
                return None
            
            # ìƒˆë¡œìš´ ë°°ì¹˜ ì‘ì—…ìœ¼ë¡œ ì¬ì‹œë„
            new_batch_job = BatchJob(
                job_type=batch_job.job_type,
                total_items=batch_job.total_items,
                metadata=batch_job.metadata.copy()
            )
            
            self.batch_jobs[str(new_batch_job.batch_job_id)] = new_batch_job
            
            # ì¬ì‹œë„ ì‹¤í–‰
            return self.execute_batch_job(str(new_batch_job.batch_job_id), processing_function, *args, **kwargs)
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì‘ì—… ì¬ì‹œë„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def get_batch_processing_statistics(self) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        total_jobs = len(self.batch_jobs)
        completed_jobs = len(self.get_completed_batch_jobs())
        failed_jobs = len(self.get_failed_batch_jobs())
        running_jobs = len(self.get_running_batch_jobs())
        
        # íƒ€ì…ë³„ í†µê³„
        type_counts = {}
        for job_type in BatchJobType:
            type_counts[job_type.value] = len(self.get_batch_jobs_by_type(job_type))
        
        # ì„±ê³µë¥  ê³„ì‚°
        success_rate = (completed_jobs / total_jobs * 100) if total_jobs > 0 else 0.0
        
        return {
            "total_jobs": total_jobs,
            "completed": completed_jobs,
            "failed": failed_jobs,
            "running": running_jobs,
            "success_rate": success_rate,
            "type_counts": type_counts
        }
    
    def cleanup_old_jobs(self, days: int = 30) -> int:
        """ì˜¤ë˜ëœ ë°°ì¹˜ ì‘ì—… ì •ë¦¬"""
        from datetime import timedelta
        
        cutoff_date = datetime.now() - timedelta(days=days)
        old_jobs = [
            job_id for job_id, job in self.batch_jobs.items()
            if job.created_at < cutoff_date and job.is_completed()
        ]
        
        for job_id in old_jobs:
            del self.batch_jobs[job_id]
        
        logger.info(f"âœ… ì˜¤ë˜ëœ ë°°ì¹˜ ì‘ì—… {len(old_jobs)}ê°œ ì •ë¦¬ ì™„ë£Œ")
        return len(old_jobs)
