"""
Retrieval Interface
ê²€ìƒ‰ ê´€ë ¨ ê·¸ë¼ë””ì˜¤ ì¸í„°í˜ì´ìŠ¤
"""

import logging
from typing import List, Dict, Any, Tuple

logger = logging.getLogger(__name__)


class RetrievalInterface:
    """ê²€ìƒ‰ ê´€ë ¨ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, rag_service):
        self.rag_service = rag_service

    def get_sample_queries(self) -> List[str]:
        """ìƒ˜í”Œ ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡ ë°˜í™˜"""
        return [
            "í—¥ì‚¬ê³ ë‚  ì•„í‚¤í…ì²˜ëŠ” ì–´ë–»ê²Œ êµ¬í˜„ë˜ì—ˆë‚˜ìš”?",
            "RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì„± ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©í‘œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ì–´ë–¤ ê¸°ìˆ  ìŠ¤íƒì„ ì‚¬ìš©í–ˆë‚˜ìš”?"
        ]

    async def search_documents(self, query: str, top_k: int = 3) -> str:
        """ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰"""
        if not query.strip():
            return "âŒ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”"
        
        try:
            result = await self.rag_service.search_documents(
                query=query.strip(),
                top_k=top_k,
                similarity_threshold=0.1
            )
            
            if not result.get("success"):
                return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}"
            
            documents = result.get("results", [])
            if not documents:
                return "ğŸ“­ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            
            output = f"ğŸ” {len(documents)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
            for i, doc in enumerate(documents, 1):
                output += f"**{i}. ì ìˆ˜: {doc.get('similarity_score', 0):.3f}**\n"
                output += f"{doc.get('content', 'ë‚´ìš© ì—†ìŒ')[:200]}...\n\n"
            
            return output
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"âŒ ì˜¤ë¥˜: {str(e)}"

    async def search_documents_with_analysis(self, query: str, top_k: int = 3) -> Tuple[str, str, str]:
        """ìƒì„¸ ë¶„ì„ê³¼ í•¨ê»˜ ë¬¸ì„œ ê²€ìƒ‰"""
        if not query.strip():
            return "âŒ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”", "", ""
        
        try:
            result = await self.rag_service.search_documents_with_analysis(
                query=query.strip(),
                top_k=top_k,
                similarity_threshold=0.1
            )
            
            if not result.get("success"):
                return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}", "", ""
            
            # ê²€ìƒ‰ ê²°ê³¼
            documents = result.get("results", [])
            if not documents:
                return "ğŸ“­ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "", ""
            
            search_results = f"ğŸ” {len(documents)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
            for i, doc in enumerate(documents, 1):
                search_results += f"**{i}. ì ìˆ˜: {doc.get('similarity_score', 0):.3f}**\n"
                search_results += f"{doc.get('content', 'ë‚´ìš© ì—†ìŒ')[:200]}...\n\n"
            
            # ì²˜ë¦¬ ê³¼ì • ë¶„ì„
            detailed_analysis = result.get("detailed_analysis", {})
            processing_steps = detailed_analysis.get("processing_steps", {})
            vector_info = detailed_analysis.get("vector_info", {})
            
            processing_info = f"â±ï¸ **ì²˜ë¦¬ ë¶„ì„:**\n"
            processing_info += f"â€¢ ì „ì²˜ë¦¬: {processing_steps.get('preprocessing', 0):.3f}s\n"
            processing_info += f"â€¢ ë²¡í„°í™”: {processing_steps.get('vectorization', 0):.3f}s\n"
            processing_info += f"â€¢ ìœ ì‚¬ë„ ê³„ì‚°: {processing_steps.get('similarity_calculation', 0):.3f}s\n"
            processing_info += f"â€¢ ì •ë ¬: {processing_steps.get('sorting', 0):.3f}s\n"
            processing_info += f"â€¢ ê²°ê³¼ ìƒì„±: {processing_steps.get('result_creation', 0):.3f}s\n"
            processing_info += f"â€¢ ì´ ì‹œê°„: {processing_steps.get('total_time', 0):.3f}s\n\n"
            
            # ë²¡í„° ì •ë³´
            vector_analysis = f"ğŸ”¢ **ë²¡í„° ë¶„ì„:**\n"
            vector_analysis += f"â€¢ ë²¡í„° ì°¨ì›: {vector_info.get('dimensions', 0)}\n"
            vector_analysis += f"â€¢ ì´ ì²­í¬ ìˆ˜: {vector_info.get('total_chunks', 0)}\n"
            vector_analysis += f"â€¢ ì²˜ë¦¬ëœ ì²­í¬: {vector_info.get('processed_chunks', 0)}\n"
            vector_analysis += f"â€¢ ìœ ì‚¬ë„ ì„ê³„ê°’: {vector_info.get('threshold_applied', 0)}\n\n"
            
            # ìœ ì‚¬ë„ ë¶„í¬
            similarity_dist = detailed_analysis.get("similarity_distribution", {})
            vector_analysis += f"ğŸ“Š **ìœ ì‚¬ë„ ë¶„í¬:**\n"
            vector_analysis += f"â€¢ ì •í™•íˆ ì¼ì¹˜: {similarity_dist.get('exact_matches', 0)}\n"
            vector_analysis += f"â€¢ ìœ ì‚¬ë„ ì¼ì¹˜: {similarity_dist.get('similarity_matches', 0)}\n"
            vector_analysis += f"â€¢ ë¬¸ë§¥ìƒ ì¼ì¹˜: {similarity_dist.get('contextual_matches', 0)}\n"
            
            return search_results, processing_info, vector_analysis
                
        except Exception as e:
            logger.error(f"ìƒì„¸ ë¶„ì„ê³¼ í•¨ê»˜ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"âŒ ì˜¤ë¥˜: {str(e)}", "", ""

    async def demonstrate_retriever_process(self, query: str) -> Tuple[str, str, str]:
        """ë¦¬íŠ¸ë¦¬ë²„ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì‹œì—°"""
        if not query.strip():
            return "âŒ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”", "", ""
        
        try:
            # 1ë‹¨ê³„: ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            step1_info = "ğŸ”„ **1ë‹¨ê³„: ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±**\n"
            step1_info += f"â€¢ ì¿¼ë¦¬: '{query}'\n"
            step1_info += f"â€¢ ëª¨ë¸: sentence-transformers/all-MiniLM-L6-v2\n"
            step1_info += f"â€¢ ë²¡í„° ì°¨ì›: 384\n"
            
            # 2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰
            step2_info = "ğŸ” **2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰**\n"
            step2_info += f"â€¢ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ + BM25\n"
            step2_info += f"â€¢ ê²€ìƒ‰ ë²”ìœ„: ì „ì²´ ë²¡í„° ìŠ¤í† ì–´\n"
            
            # ì‹¤ì œ ê²€ìƒ‰ ì‹¤í–‰
            result = await self.rag_service.search_documents_with_analysis(
                query=query.strip(),
                top_k=5,
                similarity_threshold=0.1
            )
            
            if not result.get("success"):
                return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}", "", ""
            
            documents = result.get("results", [])
            detailed_analysis = result.get("detailed_analysis", {})
            processing_steps = detailed_analysis.get("processing_steps", {})
            
            # 3ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼
            step3_info = "ğŸ“Š **3ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼**\n"
            step3_info += f"â€¢ ì°¾ì€ ë¬¸ì„œ: {len(documents)}ê°œ\n"
            step3_info += f"â€¢ ì²˜ë¦¬ ì‹œê°„: {processing_steps.get('total_time', 0):.3f}s\n\n"
            
            for i, doc in enumerate(documents[:3], 1):
                step3_info += f"**{i}. ìœ ì‚¬ë„: {doc.get('similarity_score', 0):.3f}**\n"
                step3_info += f"{doc.get('content', '')[:150]}...\n\n"
            
            # ìƒì„¸ ë¶„ì„ ì •ë³´
            analysis_info = "ğŸ”¬ **ìƒì„¸ ë¶„ì„**\n"
            analysis_info += f"â€¢ ì „ì²˜ë¦¬: {processing_steps.get('preprocessing', 0):.3f}s\n"
            analysis_info += f"â€¢ ë²¡í„°í™”: {processing_steps.get('vectorization', 0):.3f}s\n"
            analysis_info += f"â€¢ ìœ ì‚¬ë„ ê³„ì‚°: {processing_steps.get('similarity_calculation', 0):.3f}s\n"
            analysis_info += f"â€¢ ì •ë ¬: {processing_steps.get('sorting', 0):.3f}s\n"
            
            vector_info = detailed_analysis.get("vector_info", {})
            analysis_info += f"â€¢ ë²¡í„° ì°¨ì›: {vector_info.get('dimensions', 384)}\n"
            analysis_info += f"â€¢ ì´ ì²­í¬ ìˆ˜: {vector_info.get('total_chunks', 0)}\n"
            analysis_info += f"â€¢ ì²˜ë¦¬ëœ ì²­í¬: {vector_info.get('processed_chunks', 0)}\n"
            
            return step1_info, step2_info + step3_info, analysis_info
                
        except Exception as e:
            logger.error(f"ë¦¬íŠ¸ë¦¬ë²„ ê³¼ì • ì‹œì—° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"âŒ ì˜¤ë¥˜: {str(e)}", "", ""
