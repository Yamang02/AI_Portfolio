"""
Generation Interface
ë‹µë³€ ìƒì„± ê´€ë ¨ ê·¸ë¼ë””ì˜¤ ì¸í„°í˜ì´ìŠ¤
"""

import logging
from typing import List, Dict, Any, Tuple

logger = logging.getLogger(__name__)


class GenerationInterface:
    """ë‹µë³€ ìƒì„± ê´€ë ¨ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, rag_service):
        self.rag_service = rag_service

    async def generate_answer(self, question: str, max_results: int = 3) -> Tuple[str, str]:
        """ì¶œì²˜ì™€ í•¨ê»˜ RAG ë‹µë³€ ìƒì„±"""
        if not question.strip():
            return "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”", ""
        
        try:
            result = await self.rag_service.generate_rag_answer(
                question=question.strip(),
                context_hint=None,
                metadata={"timestamp": "demo"}
            )
            
            # Format answer
            answer = f"ğŸ¤– **ë‹µë³€:**\n{result.answer}\n\n"
            answer += f"â±ï¸ **ì²˜ë¦¬ ì‹œê°„:** {result.processing_time_ms:.0f}ms\n"
            answer += f"ğŸ¯ **ì‹ ë¢°ë„:** {result.confidence:.2f}"
            
            # Format sources
            if result.sources:
                sources = "ğŸ“š **ì‚¬ìš©ëœ ì¶œì²˜:**\n\n"
                for i, source in enumerate(result.sources, 1):
                    sources += f"**{i}. ìœ ì‚¬ë„: {source.similarity_score:.3f}**\n"
                    sources += f"{source.chunk.content[:300]}...\n\n"
            else:
                sources = "ğŸ“­ ì¶œì²˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            
            return answer, sources
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"âŒ ì˜¤ë¥˜: {str(e)}", ""

    async def add_document_with_analysis(self, content: str, source: str = "manual_input") -> Tuple[str, str, str]:
        """ìƒì„¸ ë¶„ì„ê³¼ í•¨ê»˜ ë¬¸ì„œ ì¶”ê°€"""
        if not content.strip():
            return "âŒ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”", "", ""
        
        try:
            result = await self.rag_service.add_document_with_analysis(
                content=content.strip(),
                source=source,
                metadata={"timestamp": "demo"}
            )
            
            if not result.get("success"):
                return f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}", "", ""
            
            # ê¸°ë³¸ ê²°ê³¼
            basic_result = f"âœ… ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!\në¬¸ì„œ ID: {result.get('document_id', 'N/A')}\nì¶œì²˜: {result.get('source', 'N/A')}"
            
            # ì²˜ë¦¬ ê³¼ì • ë¶„ì„
            processing_steps = result.get("processing_steps", {})
            vector_result = result.get("vector_result", {})
            
            processing_info = f"â±ï¸ **ì²˜ë¦¬ ë¶„ì„:**\n"
            processing_info += f"â€¢ ëª¨ë¸ ìƒì„±: {processing_steps.get('model_creation', 0):.3f}s\n"
            processing_info += f"â€¢ ë²¡í„° ì²˜ë¦¬: {processing_steps.get('vector_processing', 0):.3f}s\n"
            processing_info += f"â€¢ ì´ ì‹œê°„: {processing_steps.get('total_time', 0):.3f}s\n\n"
            
            # ë²¡í„° ì²˜ë¦¬ ê²°ê³¼
            if vector_result.get("success"):
                vector_info = f"ğŸ”¢ **ë²¡í„° ë¶„ì„:**\n"
                vector_info += f"â€¢ ìƒì„±ëœ ì²­í¬: {vector_result.get('chunks_created', 0)}\n"
                vector_info += f"â€¢ ë²¡í„° ì°¨ì›: {vector_result.get('vector_dimensions', 0)}\n"
                vector_info += f"â€¢ ì´ ë¬¸ì„œ ìˆ˜: {vector_result.get('total_documents', 0)}\n"
                vector_info += f"â€¢ ì´ ì²­í¬ ìˆ˜: {vector_result.get('total_chunks', 0)}\n\n"
                
                # ì²­í¬ ìƒì„¸ ì •ë³´
                chunk_details = vector_result.get("chunk_details", [])
                if chunk_details:
                    vector_info += "ğŸ“„ **ì²­í¬ ìƒì„¸ ì •ë³´:**\n"
                    for i, chunk in enumerate(chunk_details, 1):
                        vector_info += f"â€¢ ì²­í¬ {i}: {chunk['length']} chars - {chunk['content_preview']}\n"
            else:
                vector_info = "âŒ ë²¡í„° ì²˜ë¦¬ ì‹¤íŒ¨"
            
            return basic_result, processing_info, vector_info
                
        except Exception as e:
            logger.error(f"ìƒì„¸ ë¶„ì„ê³¼ í•¨ê»˜ ë¬¸ì„œ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"âŒ ì˜¤ë¥˜: {str(e)}", "", ""

    async def add_sample_data_to_knowledge_base(self, sample_data: List[Dict[str, Any]]) -> str:
        """ë¡œë“œëœ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì§€ì‹ ë² ì´ìŠ¤ì— ì¶”ê°€"""
        if not sample_data:
            return "<div style='text-align: center; color: #dc3545; padding: 20px; font-weight: 600;'>âŒ ë¨¼ì € ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.</div>"
        
        try:
            results = []
            for data in sample_data:
                try:
                    result = await self.rag_service.add_document_from_text(
                        content=data["content"],
                        source=data["source"],
                        metadata={"title": data["title"], "type": "sample_data"}
                    )
                    if result.get("success"):
                        results.append(f"âœ… {data['title']} ì¶”ê°€ ì™„ë£Œ")
                    else:
                        results.append(f"âŒ {data['title']} ì¶”ê°€ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                except Exception as e:
                    results.append(f"âŒ {data['title']} ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            
            # HTMLë¡œ í¬ë§·íŒ…
            html_result = """
            <div style="font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;">
                <h3 style="color: #2c3e50; margin-bottom: 20px;">â• ìƒ˜í”Œ ë°ì´í„° ì§€ì‹ ë² ì´ìŠ¤ ì¶”ê°€ ê²°ê³¼</h3>
                <div style="background: linear-gradient(135deg, #e8f5e8 0%, #f0f8f0 100%); border: 2px solid #4caf50; border-radius: 12px; padding: 20px;">
            """
            
            for line in results:
                if line.startswith('âœ…'):
                    html_result += f'<div style="color: #28a745; margin-bottom: 8px;">{line}</div>'
                elif line.startswith('âŒ'):
                    html_result += f'<div style="color: #dc3545; margin-bottom: 8px;">{line}</div>'
                else:
                    html_result += f'<div style="color: #6c757d; margin-bottom: 8px;">{line}</div>'
            
            html_result += """
                </div>
            </div>
            """
            
            return html_result
            
        except Exception as e:
            logger.error(f"ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"<div style='text-align: center; color: #dc3545; padding: 20px; font-weight: 600;'>âŒ ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: {str(e)}</div>"

    async def demonstrate_complete_rag_pipeline(self, content: str, query: str) -> Tuple[str, str, str, str]:
        """ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹œì—°: ë¬¸ì„œ ì¶”ê°€ë¶€í„° ê²€ìƒ‰ê¹Œì§€"""
        try:
            pipeline_log = []
            
            # === 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”© ===
            pipeline_log.append("ğŸ”„ **1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”©**")
            pipeline_log.append(f"â€¢ ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(content)} ë¬¸ì")
            pipeline_log.append(f"â€¢ ë¬¸ì„œ íƒ€ì…: í…ìŠ¤íŠ¸")
            pipeline_log.append(f"â€¢ ì²˜ë¦¬ ì‹œê°„: ì¦‰ì‹œ\n")
            
            # === 2ë‹¨ê³„: ë¬¸ì„œ ì €ì¥ ë° ë²¡í„°í™” ===
            pipeline_log.append("ğŸ”„ **2ë‹¨ê³„: ë¬¸ì„œ ì €ì¥ ë° ë²¡í„°í™”**")
            add_result = await self.rag_service.add_document_with_analysis(
                content=content.strip(),
                source="pipeline_demo",
                metadata={"demo": "complete_pipeline"}
            )
            
            if not add_result.get("success"):
                return "âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨", "", "", ""
            
            processing_steps = add_result.get("processing_steps", {})
            vector_result = add_result.get("vector_result", {})
            
            pipeline_log.append(f"â€¢ ì„ë² ë”© ëª¨ë¸: sentence-transformers/all-MiniLM-L6-v2")
            pipeline_log.append(f"â€¢ ë²¡í„° ì°¨ì›: {vector_result.get('vector_dimensions', 384)}")
            pipeline_log.append(f"â€¢ ìƒì„±ëœ ì²­í¬: {vector_result.get('chunks_created', 0)}ê°œ")
            pipeline_log.append(f"â€¢ ë²¡í„°í™” ì‹œê°„: {processing_steps.get('vector_processing', 0):.3f}s")
            pipeline_log.append(f"â€¢ BM25 ì¸ë±ì‹± ì™„ë£Œ\n")
            
            # === 3ë‹¨ê³„: ì¿¼ë¦¬ ì²˜ë¦¬ ===
            pipeline_log.append("ğŸ” **3ë‹¨ê³„: ì¿¼ë¦¬ ì²˜ë¦¬**")
            pipeline_log.append(f"â€¢ ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
            pipeline_log.append(f"â€¢ ì¿¼ë¦¬ ê¸¸ì´: {len(query)} ë¬¸ì")
            pipeline_log.append(f"â€¢ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜: í•˜ì´ë¸Œë¦¬ë“œ (Vector + BM25)\n")
            
            # === 4ë‹¨ê³„: ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰ ===
            search_result = await self.rag_service.search_documents_with_analysis(
                query=query.strip(),
                top_k=3,
                similarity_threshold=0.1
            )
            
            if not search_result.get("success"):
                return "\n".join(pipeline_log), "âŒ ê²€ìƒ‰ ì‹¤íŒ¨", "", ""
            
            documents = search_result.get("results", [])
            detailed_analysis = search_result.get("detailed_analysis", {})
            processing_steps_search = detailed_analysis.get("processing_steps", {})
            
            pipeline_log.append("ğŸ“Š **4ë‹¨ê³„: ê²€ìƒ‰ ì‹¤í–‰ ê²°ê³¼**")
            pipeline_log.append(f"â€¢ ì°¾ì€ ë¬¸ì„œ: {len(documents)}ê°œ")
            pipeline_log.append(f"â€¢ ê²€ìƒ‰ ì‹œê°„: {processing_steps_search.get('total_time', 0):.3f}s")
            pipeline_log.append(f"â€¢ ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°: {processing_steps_search.get('similarity_calculation', 0):.3f}s")
            pipeline_log.append(f"â€¢ BM25 ì ìˆ˜ ê³„ì‚°: {processing_steps_search.get('preprocessing', 0):.3f}s")
            
            # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
            search_results = f"ğŸ” **ê²€ìƒ‰ ê²°ê³¼ ({len(documents)}ê°œ)**\n\n"
            for i, doc in enumerate(documents, 1):
                search_results += f"**{i}. ìœ ì‚¬ë„: {doc.get('similarity_score', 0):.3f}**\n"
                search_results += f"{doc.get('content', '')[:300]}...\n\n"
            
            # ë²¡í„° ë¶„ì„ ì •ë³´
            vector_info = detailed_analysis.get("vector_info", {})
            vector_analysis = f"ğŸ”¢ **ë²¡í„° ë¶„ì„**\n"
            vector_analysis += f"â€¢ ì²˜ë¦¬ëœ ì²­í¬: {vector_info.get('processed_chunks', 0)}ê°œ\n"
            vector_analysis += f"â€¢ ë²¡í„° ì°¨ì›: {vector_info.get('dimensions', 384)}\n"
            vector_analysis += f"â€¢ ìœ ì‚¬ë„ ì„ê³„ê°’: {vector_info.get('threshold_applied', 0.1)}\n\n"
            
            similarity_dist = detailed_analysis.get("similarity_distribution", {})
            vector_analysis += f"**ìœ ì‚¬ë„ ë¶„í¬:**\n"
            vector_analysis += f"â€¢ ê³ ìœ ì‚¬ë„ (>0.7): {similarity_dist.get('exact_matches', 0)}ê°œ\n"
            vector_analysis += f"â€¢ ì¤‘ìœ ì‚¬ë„ (0.3-0.7): {similarity_dist.get('similarity_matches', 0)}ê°œ\n"
            vector_analysis += f"â€¢ ì €ìœ ì‚¬ë„ (<0.3): {similarity_dist.get('contextual_matches', 0)}ê°œ\n"
            
            # === 5ë‹¨ê³„: RAG ë‹µë³€ ìƒì„± ===
            if documents:
                rag_result = await self.rag_service.generate_rag_answer(
                    question=query.strip(),
                    context_hint=None,
                    metadata={"demo": "complete_pipeline"}
                )
                
                pipeline_log.append(f"\nğŸ¤– **5ë‹¨ê³„: RAG ë‹µë³€ ìƒì„±**")
                pipeline_log.append(f"â€¢ LLM ëª¨ë¸: MockLLM (ë°ëª¨ìš©)")
                pipeline_log.append(f"â€¢ ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸: {len(rag_result.sources)}ê°œ ë¬¸ì„œ")
                pipeline_log.append(f"â€¢ ë‹µë³€ ìƒì„± ì‹œê°„: {rag_result.processing_time_ms:.0f}ms")
                pipeline_log.append(f"â€¢ ì‹ ë¢°ë„: {rag_result.confidence:.2f}")
                
                final_answer = f"ğŸ¤– **ìµœì¢… RAG ë‹µë³€**\n\n{rag_result.answer}\n\n"
                final_answer += f"**ë©”íƒ€ ì •ë³´:**\n"
                final_answer += f"â€¢ ì²˜ë¦¬ ì‹œê°„: {rag_result.processing_time_ms:.0f}ms\n"
                final_answer += f"â€¢ ì‹ ë¢°ë„: {rag_result.confidence:.2f}\n"
                final_answer += f"â€¢ ì‚¬ìš©ëœ ì†ŒìŠ¤: {len(rag_result.sources)}ê°œ"
            else:
                pipeline_log.append(f"\nâŒ **5ë‹¨ê³„: RAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨**")
                pipeline_log.append("â€¢ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                final_answer = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return "\n".join(pipeline_log), search_results, vector_analysis, final_answer
            
        except Exception as e:
            logger.error(f"ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹œì—° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"âŒ ì˜¤ë¥˜: {str(e)}", "", "", ""
